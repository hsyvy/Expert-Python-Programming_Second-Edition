{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aptos_binary_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsyvy/Expert-Python-Programming_Second-Edition/blob/master/aptos_binary_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81a1GFDrY7fB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ibDY5iY8h0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/drive/My\\ Drive/model_logs/aptos_binary/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4Wdm5UqalYT",
        "colab_type": "code",
        "outputId": "6fbce125-3881-4594-ac64-438fbb5a7d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8gS5fmwQwek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir model_logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-HZHffV44S6",
        "colab_type": "code",
        "outputId": "53f0a5a5-9763-42bf-b7aa-19f70a939854",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-637cb624-64cd-4965-83e5-bf40cb8af156\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-637cb624-64cd-4965-83e5-bf40cb8af156\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Saving se_resnet.py to se_resnet.py\n",
            "Saving cn_se_resnet.py to cn_se_resnet.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cn_se_resnet.py': b'import torch\\nimport torch.nn as nn\\n\\n\\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\\n\\n\\ndef conv1x1(in_planes, out_planes, stride=1):\\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\\n\\nclass SeBlock(nn.Module):\\n\\n    def __init__(self, inplanes, reduction=1):\\n        super(SeBlock, self).__init__()\\n\\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\\n        self.fc1 = nn.Conv2d(inplanes, inplanes // reduction, kernel_size=1)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.fc2 = nn.Conv2d(inplanes // reduction, inplanes, kernel_size=1)\\n        self.sigmoid = nn.Sigmoid()\\n\\n    def forward(self, x):\\n        out = self.avg_pool(x)\\n        out = self.fc1(out)\\n        out = self.relu(out)\\n        out = self.fc2(out)\\n        out = self.relu(out)\\n        out = self.sigmoid(out)\\n\\n        return out * x\\n\\n\\nclass SeBasicBlock(nn.Module):\\n    expansion = 1\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\\n                 base_width=64, dilation=1, norm_layer=None):\\n        super(SeBasicBlock, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        if groups != 1 or base_width != 64:\\n            raise ValueError(\\'SeBasicBlock only supports groups=1 and base_width=64\\')\\n        if dilation > 1:\\n            raise NotImplementedError(\"Dilation > 1 not supported in SeBasicBlock\")\\n\\n        self.conv1 = conv3x3(inplanes, planes, stride)\\n        self.bn1 = norm_layer(planes)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.conv2 = conv3x3(planes, planes)\\n        self.bn2 = norm_layer(planes)\\n        self.se_layer = SeBlock(planes)\\n        self.downsample = downsample\\n        self.stride = stride\\n\\n    def forward(self, x):\\n        identity = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n\\n        if self.downsample is not None:\\n            identity = self.downsample(x)\\n\\n        out += identity\\n        out = self.relu(out)\\n\\n        return out\\n\\n\\nclass SeBottleneck(nn.Module):\\n    expansion = 4\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\\n                 base_width=64, dilation=1, norm_layer=None):\\n        super(SeBottleneck, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        width = int(planes * (base_width / 64.)) * groups\\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\\n        self.conv1 = conv1x1(inplanes, width)\\n        self.bn1 = norm_layer(width)\\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\\n        self.bn2 = norm_layer(width)\\n        self.conv3 = conv1x1(width, planes * self.expansion)\\n        self.bn3 = norm_layer(planes * self.expansion)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.se_layer = SeBlock(planes*self.expansion)\\n        self.downsample = downsample\\n        self.stride = stride\\n\\n    def forward(self, x):\\n        identity = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n        out = self.relu(out)\\n\\n        out = self.conv3(out)\\n        out = self.bn3(out)\\n        out = self.relu(out)\\n\\n        out = self.se_layer(out)\\n\\n        if self.downsample is not None:\\n            identity = self.downsample(x)\\n\\n        out += identity\\n        out = self.relu(out)\\n\\n        return out\\n\\n\\nclass CnSeResNet(nn.Module):\\n\\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\\n                 norm_layer=None):\\n        super(CnSeResNet, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        self._norm_layer = norm_layer\\n\\n        self.inplanes = 64\\n        self.dilation = 1\\n        if replace_stride_with_dilation is None:\\n            # each element in the tuple indicates if we should replace\\n            # the 2x2 stride with a dilated convolution instead\\n            replace_stride_with_dilation = [False, False, False]\\n        if len(replace_stride_with_dilation) != 3:\\n            raise ValueError(\"replace_stride_with_dilation should be None \"\\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\\n        self.groups = groups\\n        self.base_width = width_per_group\\n        self.conv0 = nn.Conv3d(1, 1, kernel_size=(3, 1, 1))\\n        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\\n                               bias=False)\\n        self.bn1 = norm_layer(self.inplanes)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\\n        self.layer1 = self._make_layer(block, 64, layers[0])\\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\\n                                       dilate=replace_stride_with_dilation[0])\\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\\n                                       dilate=replace_stride_with_dilation[1])\\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\\n                                       dilate=replace_stride_with_dilation[2])\\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\\n\\n        for m in self.modules():\\n            if isinstance(m, nn.Conv2d):\\n                nn.init.kaiming_normal_(m.weight, mode=\\'fan_out\\', nonlinearity=\\'relu\\')\\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\\n                nn.init.constant_(m.weight, 1)\\n                nn.init.constant_(m.bias, 0)\\n\\n        # Zero-initialize the last BN in each residual branch,\\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\\n        if zero_init_residual:\\n            for m in self.modules():\\n                if isinstance(m, SeBottleneck):\\n                    nn.init.constant_(m.bn3.weight, 0)\\n                elif isinstance(m, SeBasicBlock):\\n                    nn.init.constant_(m.bn2.weight, 0)\\n\\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\\n        norm_layer = self._norm_layer\\n        downsample = None\\n        previous_dilation = self.dilation\\n        if dilate:\\n            self.dilation *= stride\\n            stride = 1\\n        if stride != 1 or self.inplanes != planes * block.expansion:\\n            downsample = nn.Sequential(\\n                conv1x1(self.inplanes, planes * block.expansion, stride),\\n                norm_layer(planes * block.expansion),\\n            )\\n\\n        layers = list()\\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\\n                            self.base_width, previous_dilation, norm_layer))\\n        self.inplanes = planes * block.expansion\\n        for _ in range(1, blocks):\\n            layers.append(block(self.inplanes, planes, groups=self.groups,\\n                                base_width=self.base_width, dilation=self.dilation,\\n                                norm_layer=norm_layer))\\n\\n        return nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        x = self.conv0(x)\\n        x = x.view(-1, 1, 512, 512)\\n        x = self.conv1(x)\\n        x = self.bn1(x)\\n        x = self.relu(x)\\n        x = self.maxpool(x)\\n\\n        x = self.layer1(x)\\n        x = self.layer2(x)\\n        x = self.layer3(x)\\n        x = self.layer4(x)\\n\\n        x = self.avgpool(x)\\n        x = torch.flatten(x, 1)\\n        x = self.fc(x)\\n\\n        return x\\n\\n\\ndef _cn_se_resnet(block, layers, **kwargs):\\n    model = CnSeResNet(block, layers, **kwargs)\\n    return model\\n\\n\\ndef cn_se_resnet50(**kwargs):\\n    return _cn_se_resnet(SeBottleneck, [3, 4, 6, 3], **kwargs)\\n\\n\\ndef cn_se_resnet101(**kwargs):\\n    return _cn_se_resnet(SeBottleneck, [3, 4, 23, 3], **kwargs)\\n\\n\\ndef cn_se_resnet152(**kwargs):\\n\\n    return _cn_se_resnet(SeBottleneck, [3, 8, 36, 3], **kwargs)\\n\\n\\ndef se_resnext50_32x4d(**kwargs):\\n\\n    kwargs[\\'groups\\'] = 32\\n    kwargs[\\'width_per_group\\'] = 4\\n    return _cn_se_resnet(SeBottleneck, [3, 4, 6, 3], **kwargs)\\n\\n\\ndef se_resnext101_32x8d(**kwargs):\\n    kwargs[\\'groups\\'] = 32\\n    kwargs[\\'width_per_group\\'] = 8\\n    return _cn_se_resnet(SeBottleneck, [3, 4, 23, 3], **kwargs)\\n\\n\\ndef wide_cn_se_resnet50_2(**kwargs):\\n    kwargs[\\'width_per_group\\'] = 64 * 2\\n    return _cn_se_resnet(SeBottleneck, [3, 4, 6, 3], **kwargs)\\n\\n\\ndef wide_cn_se_resnet101_2(**kwargs):\\n    kwargs[\\'width_per_group\\'] = 64 * 2\\n    return _cn_se_resnet(SeBottleneck, [3, 4, 23, 3], **kwargs)\\n\\n',\n",
              " 'kaggle.json': b'{\"username\":\"hsyvy7\",\"key\":\"447978a79bd0d0061d5869faad504b65\"}',\n",
              " 'se_resnet.py': b'import torch\\nimport torch.nn as nn\\n\\n\\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\\n\\n\\ndef conv1x1(in_planes, out_planes, stride=1):\\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\\n\\nclass SeBlock(nn.Module):\\n\\n    def __init__(self, inplanes, reduction=1):\\n        super(SeBlock, self).__init__()\\n\\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\\n        self.fc1 = nn.Conv2d(inplanes, inplanes // reduction, kernel_size=1)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.fc2 = nn.Conv2d(inplanes // reduction, inplanes, kernel_size=1)\\n        self.sigmoid = nn.Sigmoid()\\n\\n    def forward(self, x):\\n        out = self.avg_pool(x)\\n        out = self.fc1(out)\\n        out = self.relu(out)\\n        out = self.fc2(out)\\n        out = self.relu(out)\\n        out = self.sigmoid(out)\\n\\n        return out * x\\n\\n\\nclass SeBasicBlock(nn.Module):\\n    expansion = 1\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\\n                 base_width=64, dilation=1, norm_layer=None):\\n        super(SeBasicBlock, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        if groups != 1 or base_width != 64:\\n            raise ValueError(\\'SeBasicBlock only supports groups=1 and base_width=64\\')\\n        if dilation > 1:\\n            raise NotImplementedError(\"Dilation > 1 not supported in SeBasicBlock\")\\n\\n        self.conv1 = conv3x3(inplanes, planes, stride)\\n        self.bn1 = norm_layer(planes)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.conv2 = conv3x3(planes, planes)\\n        self.bn2 = norm_layer(planes)\\n        self.se_layer = SeBlock(planes)\\n        self.downsample = downsample\\n        self.stride = stride\\n\\n    def forward(self, x):\\n        identity = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n\\n        if self.downsample is not None:\\n            identity = self.downsample(x)\\n\\n        out += identity\\n        out = self.relu(out)\\n\\n        return out\\n\\n\\nclass SeBottleneck(nn.Module):\\n    expansion = 4\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\\n                 base_width=64, dilation=1, norm_layer=None):\\n        super(SeBottleneck, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        width = int(planes * (base_width / 64.)) * groups\\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\\n        self.conv1 = conv1x1(inplanes, width)\\n        self.bn1 = norm_layer(width)\\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\\n        self.bn2 = norm_layer(width)\\n        self.conv3 = conv1x1(width, planes * self.expansion)\\n        self.bn3 = norm_layer(planes * self.expansion)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.se_layer = SeBlock(planes*self.expansion)\\n        self.downsample = downsample\\n        self.stride = stride\\n\\n    def forward(self, x):\\n        identity = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n        out = self.relu(out)\\n\\n        out = self.conv3(out)\\n        out = self.bn3(out)\\n        out = self.relu(out)\\n        \\n        out = self.se_layer(out)\\n\\n        if self.downsample is not None:\\n            identity = self.downsample(x)\\n\\n        out += identity\\n        out = self.relu(out)\\n\\n        return out\\n\\n\\nclass SeResNet(nn.Module):\\n\\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\\n                 norm_layer=None):\\n        super(SeResNet, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        self._norm_layer = norm_layer\\n\\n        self.inplanes = 64\\n        self.dilation = 1\\n        if replace_stride_with_dilation is None:\\n            # each element in the tuple indicates if we should replace\\n            # the 2x2 stride with a dilated convolution instead\\n            replace_stride_with_dilation = [False, False, False]\\n        if len(replace_stride_with_dilation) != 3:\\n            raise ValueError(\"replace_stride_with_dilation should be None \"\\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\\n        self.groups = groups\\n        self.base_width = width_per_group\\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\\n                               bias=False)\\n        self.bn1 = norm_layer(self.inplanes)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\\n        self.layer1 = self._make_layer(block, 64, layers[0])\\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\\n                                       dilate=replace_stride_with_dilation[0])\\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\\n                                       dilate=replace_stride_with_dilation[1])\\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\\n                                       dilate=replace_stride_with_dilation[2])\\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\\n\\n        for m in self.modules():\\n            if isinstance(m, nn.Conv2d):\\n                nn.init.kaiming_normal_(m.weight, mode=\\'fan_out\\', nonlinearity=\\'relu\\')\\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\\n                nn.init.constant_(m.weight, 1)\\n                nn.init.constant_(m.bias, 0)\\n\\n        # Zero-initialize the last BN in each residual branch,\\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\\n        if zero_init_residual:\\n            for m in self.modules():\\n                if isinstance(m, SeBottleneck):\\n                    nn.init.constant_(m.bn3.weight, 0)\\n                elif isinstance(m, SeBasicBlock):\\n                    nn.init.constant_(m.bn2.weight, 0)\\n\\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\\n        norm_layer = self._norm_layer\\n        downsample = None\\n        previous_dilation = self.dilation\\n        if dilate:\\n            self.dilation *= stride\\n            stride = 1\\n        if stride != 1 or self.inplanes != planes * block.expansion:\\n            downsample = nn.Sequential(\\n                conv1x1(self.inplanes, planes * block.expansion, stride),\\n                norm_layer(planes * block.expansion),\\n            )\\n\\n        layers = list()\\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\\n                            self.base_width, previous_dilation, norm_layer))\\n        self.inplanes = planes * block.expansion\\n        for _ in range(1, blocks):\\n            layers.append(block(self.inplanes, planes, groups=self.groups,\\n                                base_width=self.base_width, dilation=self.dilation,\\n                                norm_layer=norm_layer))\\n\\n        return nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.bn1(x)\\n        x = self.relu(x)\\n        x = self.maxpool(x)\\n\\n        x = self.layer1(x)\\n        x = self.layer2(x)\\n        x = self.layer3(x)\\n        x = self.layer4(x)\\n\\n        x = self.avgpool(x)\\n        x = torch.flatten(x, 1)\\n        x = self.fc(x)\\n\\n        return x\\n\\n\\ndef _se_resnet(block, layers, **kwargs):\\n    model = SeResNet(block, layers, **kwargs)\\n    return model\\n\\n\\ndef se_resnet50(**kwargs):\\n    return _se_resnet(SeBottleneck, [3, 4, 6, 3], **kwargs)\\n\\n\\ndef se_resnet101(**kwargs):\\n    return _se_resnet(SeBottleneck, [3, 4, 23, 3], **kwargs)\\n\\n\\ndef se_resnet152(**kwargs):\\n\\n    return _se_resnet(SeBottleneck, [3, 8, 36, 3], **kwargs)\\n\\n\\ndef se_resnext50_32x4d(**kwargs):\\n\\n    kwargs[\\'groups\\'] = 32\\n    kwargs[\\'width_per_group\\'] = 4\\n    return _se_resnet(SeBottleneck, [3, 4, 6, 3], **kwargs)\\n\\n\\ndef se_resnext101_32x8d(**kwargs):\\n    kwargs[\\'groups\\'] = 32\\n    kwargs[\\'width_per_group\\'] = 8\\n    return _se_resnet(SeBottleneck, [3, 4, 23, 3], **kwargs)\\n\\n\\ndef wide_se_resnet50_2(**kwargs):\\n    kwargs[\\'width_per_group\\'] = 64 * 2\\n    return _se_resnet(SeBottleneck, [3, 4, 6, 3], **kwargs)\\n\\n\\ndef wide_se_resnet101_2(**kwargs):\\n    kwargs[\\'width_per_group\\'] = 64 * 2\\n    return _se_resnet(SeBottleneck, [3, 4, 23, 3], **kwargs)\\n\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WsHJM8-5HRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj5FAZEp5KH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c aptos2019-blindness-detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYUSvD2z5Q2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir train\n",
        "!mkdir test\n",
        "!unzip train_images.zip -d train\n",
        "!unzip test_images.zip -d test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmWDUWLI6TT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('train.csv')\n",
        "train['score'] = (train.diagnosis > 0)*1\n",
        "score1 = train[train.score == 1]\n",
        "score0 = train[train.score == 0]\n",
        "score1 = score1.sample(frac=1).reset_index(drop=True)\n",
        "score0 = score0.sample(frac=1).reset_index(drop=True)\n",
        "val = pd.merge(score1[:350], score0[:350], how='outer')\n",
        "train_df = pd.merge(score1[350:], score0[350:], how='outer')\n",
        "val = val.sample(frac=1).reset_index(drop=True)\n",
        "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "train_df.drop('diagnosis', axis =1, inplace=True)\n",
        "val.drop('diagnosis', axis = 1,  inplace=True)\n",
        "train_df.to_csv('train_binary.csv', index=False)\n",
        "val.to_csv('val_binary.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3bnCDaR63i_",
        "colab_type": "code",
        "outputId": "31537758-3803-435b-bf12-3baf9f4efd2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cn_se_resnet.py  sample_submission.csv\ttest_images.zip   train_images.zip\n",
            "kaggle.json\t se_resnet.py\t\ttrain\t\t  val_binary.csv\n",
            "model_logs\t test\t\t\ttrain_binary.csv\n",
            "sample_data\t test.csv\t\ttrain.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLX4B3DKMA3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cn_se_resnet import cn_se_resnet50\n",
        "model = cn_se_resnet50().cuda()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5qrBE_64ifP",
        "colab_type": "code",
        "outputId": "9c6193b6-f367-4482-8777-049eb7cda53a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "# from torchvision.models import resnet50\n",
        "# from se_resnet import se_resnext50_32x4d\n",
        "from cn_se_resnet import cn_se_resnet50\n",
        "\n",
        "from torchsummary import summary\n",
        "from skimage import io, transform\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "IMG_SIZE = 512\n",
        "class AptosBlindnessDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform = None):\n",
        "        self.image_df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        folder = self.image_df.id_code.values[idx] + '.png'\n",
        "        path = os.path.join(self.root_dir, folder)\n",
        "        image = cv2.imread(path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        image = transforms.ToPILImage()(image)\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        label = self.image_df.score.values[idx]\n",
        "        \n",
        "            \n",
        "        return image, label\n",
        "    \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation((0, 360)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_data = AptosBlindnessDetectionDataset('train_binary.csv', 'train/', transform=transform_train)\n",
        "val_data = AptosBlindnessDetectionDataset('val_binary.csv', 'train/', transform=transform_test)\n",
        "\n",
        "\n",
        "\n",
        "base_model = cn_se_resnet50().cuda()\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(Model, self).__init__()\n",
        "        self.initial = base_model\n",
        "        self.fc = nn.Linear(1000, 2)\n",
        "        self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.initial(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc(out)\n",
        "        out = self.softmax(out)\n",
        "        \n",
        "        return out\n",
        "        \n",
        "model = Model(base_model).cuda()\n",
        "      \n",
        "\n",
        "N_EPOCHS = 50\n",
        "BATCH_SIZE = 12\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=0)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "    \n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0.0\n",
        "\n",
        "    trainLoader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        \n",
        "        inputs = torch.reshape(inputs, (-1, 1, 3, 512, 512))\n",
        "        \n",
        "\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        _, preds = torch.max(output, 1)\n",
        "        total_corrects += torch.sum(preds == labels.data)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        \n",
        "    total_avg_loss = total_loss/((i+1)*BATCH_SIZE)\n",
        "    train_acc = total_corrects.float()/((i+1)*BATCH_SIZE)\n",
        "    print(f'train loss: {total_avg_loss}, train acc: {train_acc}')\n",
        "\n",
        "\n",
        "def val():\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0.0\n",
        "\n",
        "    valLoader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for i, data in enumerate(valLoader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        inputs = torch.reshape(inputs, (-1, 1, 3, 512, 512))\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output, labels)\n",
        "            _, preds = torch.max(output, 1)\n",
        "            \n",
        "        total_corrects += torch.sum(preds == labels.data)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    total_avg_loss = total_loss/((i+1)*BATCH_SIZE)\n",
        "    val_acc = total_corrects.float()/((i+1)*BATCH_SIZE)\n",
        "    print(f'val loss: {total_avg_loss}, val acc: {val_acc}')\n",
        "\n",
        "    return total_avg_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_avg_loss = 10\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print('*'*50)\n",
        "    print(f'_________EPOCH {epoch}__________')\n",
        "    t0 = time.time()\n",
        "    train()\n",
        "    val_loss = val()\n",
        "    print(f'time: {(time.time()-t0)/60}')\n",
        "\n",
        "    if val_loss < best_avg_loss:\n",
        "        torch.save(model.state_dict(), f'model_logs/resnext_{epoch}_{val_loss}')\n",
        "        print(\"model is saved!!!\")\n",
        "        best_avg_loss = val_loss\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "_________EPOCH 0__________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.04634144039530503, train acc: 0.7621456980705261\n",
            "val loss: 0.03831630789459088, val acc: 0.8347457647323608\n",
            "time: 13.372220007578532\n",
            "model is saved!!!\n",
            "**************************************************\n",
            "_________EPOCH 1__________\n",
            "train loss: 0.03809547623400746, train acc: 0.8539136052131653\n",
            "val loss: 0.03658105626618121, val acc: 0.8587570786476135\n",
            "time: 13.419166572888692\n",
            "model is saved!!!\n",
            "**************************************************\n",
            "_________EPOCH 2__________\n",
            "train loss: 0.036830118149720065, train acc: 0.8636976480484009\n",
            "val loss: 0.03612709579999838, val acc: 0.8587570786476135\n",
            "time: 13.47375998099645\n",
            "model is saved!!!\n",
            "**************************************************\n",
            "_________EPOCH 3__________\n",
            "train loss: 0.03566151609023412, train acc: 0.8836032152175903\n",
            "val loss: 0.03364953095630064, val acc: 0.9067796468734741\n",
            "time: 13.496878163019817\n",
            "model is saved!!!\n",
            "**************************************************\n",
            "_________EPOCH 4__________\n",
            "train loss: 0.03558873231353065, train acc: 0.8795546293258667\n",
            "val loss: 0.035789175558898406, val acc: 0.8714689016342163\n",
            "time: 13.500581506888071\n",
            "**************************************************\n",
            "_________EPOCH 5__________\n",
            "train loss: 0.03482461865493643, train acc: 0.8910256028175354\n",
            "val loss: 0.03380981285524907, val acc: 0.8940678238868713\n",
            "time: 13.525598649183909\n",
            "**************************************************\n",
            "_________EPOCH 6__________\n",
            "train loss: 0.0342262211216767, train acc: 0.8991227746009827\n",
            "val loss: 0.034515967350558376, val acc: 0.8855932354927063\n",
            "time: 13.503301513195037\n",
            "**************************************************\n",
            "_________EPOCH 7__________\n",
            "train loss: 0.03383402063859458, train acc: 0.9055330157279968\n",
            "val loss: 0.03239120311487866, val acc: 0.9180790781974792\n",
            "time: 13.505412459373474\n",
            "model is saved!!!\n",
            "**************************************************\n",
            "_________EPOCH 8__________\n",
            "train loss: 0.03407003218747987, train acc: 0.9031713604927063\n",
            "val loss: 0.03482623023670272, val acc: 0.8827683925628662\n",
            "time: 13.55740001598994\n",
            "**************************************************\n",
            "_________EPOCH 9__________\n",
            "train loss: 0.03346880142264038, train acc: 0.9075573086738586\n",
            "val loss: 0.03252575256056705, val acc: 0.9081920981407166\n",
            "time: 13.512200522422791\n",
            "**************************************************\n",
            "_________EPOCH 10__________\n",
            "train loss: 0.03395746682497815, train acc: 0.9021592140197754\n",
            "val loss: 0.033243010146806466, val acc: 0.9067796468734741\n",
            "time: 13.576958378156027\n",
            "**************************************************\n",
            "_________EPOCH 11__________\n",
            "train loss: 0.033508520256652526, train acc: 0.9078947305679321\n",
            "val loss: 0.03191218146328199, val acc: 0.9194915294647217\n",
            "time: 13.628014659881591\n",
            "model is saved!!!\n",
            "**************************************************\n",
            "_________EPOCH 12__________\n",
            "train loss: 0.03329971528206116, train acc: 0.9136301875114441\n",
            "val loss: 0.04365480463888686, val acc: 0.7754237055778503\n",
            "time: 13.714541153113048\n",
            "**************************************************\n",
            "_________EPOCH 13__________\n",
            "train loss: 0.03272875639711797, train acc: 0.917341411113739\n",
            "val loss: 0.03299343278684185, val acc: 0.9025423526763916\n",
            "time: 13.666709963480631\n",
            "**************************************************\n",
            "_________EPOCH 14__________\n",
            "train loss: 0.032552165299774664, train acc: 0.9197030663490295\n",
            "val loss: 0.03174896163623885, val acc: 0.9194915294647217\n",
            "time: 13.677117176850636\n",
            "model is saved!!!\n",
            "**************************************************\n",
            "_________EPOCH 15__________\n",
            "train loss: 0.03243401761238392, train acc: 0.9217273592948914\n",
            "val loss: 0.031484429824486966, val acc: 0.9251412153244019\n",
            "time: 13.741880138715109\n",
            "model is saved!!!\n",
            "**************************************************\n",
            "_________EPOCH 16__________\n",
            "train loss: 0.032205816956586364, train acc: 0.9251011610031128\n",
            "val loss: 0.03419806138943818, val acc: 0.8940678238868713\n",
            "time: 13.787497913837433\n",
            "**************************************************\n",
            "_________EPOCH 17__________\n",
            "train loss: 0.032083207849468456, train acc: 0.925775945186615\n",
            "val loss: 0.03161268908593614, val acc: 0.9180790781974792\n",
            "time: 13.789183231194814\n",
            "**************************************************\n",
            "_________EPOCH 18__________\n",
            "train loss: 0.032237322324844185, train acc: 0.9254385828971863\n",
            "val loss: 0.034124956166340134, val acc: 0.8940678238868713\n",
            "time: 13.762753518422445\n",
            "**************************************************\n",
            "_________EPOCH 19__________\n",
            "train loss: 0.03182997549513252, train acc: 0.9318488240242004\n",
            "val loss: 0.031647341694198756, val acc: 0.9251412153244019\n",
            "time: 13.68320236603419\n",
            "**************************************************\n",
            "_________EPOCH 20__________\n",
            "train loss: 0.03159133584192086, train acc: 0.9308366775512695\n",
            "val loss: 0.03225972120371242, val acc: 0.9166666865348816\n",
            "time: 13.704382153352102\n",
            "**************************************************\n",
            "_________EPOCH 21__________\n",
            "train loss: 0.03138198321446394, train acc: 0.936909556388855\n",
            "val loss: 0.03582695160208449, val acc: 0.8714689016342163\n",
            "time: 13.710470525423686\n",
            "**************************************************\n",
            "_________EPOCH 22__________\n",
            "train loss: 0.03210089575906514, train acc: 0.925775945186615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-045e9e103fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'time: {(time.time()-t0)/60}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-045e9e103fdb>\u001b[0m in \u001b[0;36mval\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mtotal_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mtotal_avg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGHsEnrxDdSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 512\n",
        "class AptosBlindnessDetectionDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform = None):\n",
        "        self.image_df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        folder = self.image_df.id_code.values[idx] + '.png'\n",
        "        path = os.path.join(self.root_dir, folder)\n",
        "        image = cv2.imread(path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        image = transforms.ToPILImage()(image)\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        label = self.image_df.score.values[idx]\n",
        "        \n",
        "            \n",
        "        return image, label\n",
        "    \n",
        "transform_pipline = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation((0, 360)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_data = AptosBlindnessDetectionDataset('train_binary.csv', 'train/', transform=transform_pipline)\n",
        "val_data = AptosBlindnessDetectionDataset('val_binary.csv', 'train/', transform=transform_pipline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqzHv-3CoyTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = resnet50().cuda()\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(Model, self).__init__()\n",
        "        self.initial = base_model\n",
        "        self.fc = nn.Linear(1000, 2)\n",
        "        self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.initial(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc(out)\n",
        "        out = self.softmax(out)\n",
        "        \n",
        "        return out\n",
        "        \n",
        "model = Model(base_model).cuda()\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du9BK5KGtAtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 15\n",
        "BATCH_SIZE = 24\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ__vZvhtkCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train():\n",
        "    \n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0.0\n",
        "\n",
        "    trainLoader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        _, preds = torch.max(output, 1)\n",
        "        total_corrects += torch.sum(preds == labels.data)\n",
        "        print(total_corrects.float())\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        \n",
        "    total_avg_loss = total_loss/((i+1)*BATCH_SIZE)\n",
        "    train_acc = total_corrects.float()/((i+1)*BATCH_SIZE)\n",
        "    print(f'train loss: {total_avg_loss}, train acc: {train_acc}')\n",
        "\n",
        "\n",
        "def val():\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0.0\n",
        "\n",
        "    valLoader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for i, data in enumerate(valLoader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        \n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output, labels)\n",
        "            _, preds = torch.max(output, 1)\n",
        "            \n",
        "        total_corrects += torch.sum(preds == labels.data)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    total_avg_loss = total_loss/((i+1)*BATCH_SIZE)\n",
        "    val_acc = total_corrects.float()/((i+1)*BATCH_SIZE)\n",
        "    print(f'val loss: {total_avg_loss}, val acc: {val_acc}')\n",
        "\n",
        "    return total_avg_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgnGuxO8ulfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_avg_loss = 10\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print('*'*50)\n",
        "    print(f'_________EPOCH {epoch}__________')\n",
        "    t0 = time.time()\n",
        "    train()\n",
        "    val_loss = val()\n",
        "    print(f'time: {(time.time()-t0)/60}')\n",
        "\n",
        "    if val_loss < best_avg_loss:\n",
        "        torch.save(model.state_dict(), f'/content/drive/My\\ Drive/model_logs/aptos_binary/model_{epoch}_{val_loss}')\n",
        "        print(\"model is saved!!!\")\n",
        "        best_avg_loss = val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69atrK3I64wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while(True):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJIUH-fMuvKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainLoader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "for i, data in enumerate(trainLoader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    print(inputs.shape)\n",
        "    print(labels.shape)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yark_Rbv4paP",
        "colab_type": "code",
        "outputId": "defaf025-299e-4931-f8a2-7c1e42fa36da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 22 15:19:47 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnOiIT0p-rmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}